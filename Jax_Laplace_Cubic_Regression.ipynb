{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93cf824c-27c3-4d8d-a349-d574d6e93fe7",
   "metadata": {},
   "source": [
    "## Using Laplace Approximation on Cubic Regression Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5610fb2-6fcf-405a-a4b5-d54c70b3e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.integrate import solve_ivp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from jax import make_jaxpr\n",
    "from jax.config import config\n",
    "from jax import value_and_grad\n",
    "from jax import grad, vmap, pmap, jit\n",
    "import jax.tree_util as jtu\n",
    "\n",
    "import optax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import flax\n",
    "\n",
    "from typing import Any, Callable, Sequence, Optional\n",
    "import sympy\n",
    "\n",
    "from sympy import Matrix\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from NN_arch import PiNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7a35a5-f532-47e9-8dd0-30f8b8cdab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#enable float64 (required)\n",
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a068a293-2083-482c-bfd1-5135885361fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\n",
    "warnings.filterwarnings(\"ignore\", \"use_inf_as_na\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d54224e-fc7c-4dfd-aa7e-baa6b8616213",
   "metadata": {},
   "source": [
    "### Define True Model Function and Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a131bafe-14d9-4009-80e4-cdfc602c819b",
   "metadata": {},
   "source": [
    "$ y = 1 + x + 2x^2 + 4x^3$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1355f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndata = 200 #number of known data points\n",
    "\n",
    "t0 = -1.25\n",
    "t1 = 1.25\n",
    "t = jnp.linspace(t0, t1, ndata)\n",
    "\n",
    "def true_fun(t):\n",
    "    return jnp.array([1 + t + 2*t**2 + 4*t**3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b71f4c-f1fc-4c99-9479-1dafb8f8f6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdev = 3\n",
    "\n",
    "seed = 989\n",
    "np.random.seed(seed)\n",
    "\n",
    "true_y = true_fun(t).squeeze() \n",
    "true_y = true_y + np.random.normal(scale=stdev, size=true_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a14265e-51bf-49ba-8e57-cae031e2164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the dataset\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.scatter(t, true_y.squeeze(), label='Noise Corrupted Data')\n",
    "plt.plot(t, true_fun(t).squeeze() , color='g', label=\"True Model\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291a8ba6-0cfd-49b5-89e3-d57edb3f6672",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4596f0f-159e-4841-8b64-d265d6e8763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_y = true_y[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70cb6a7-1d74-4136-ac2b-bfff9ba2fa83",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038584dd-8066-4207-9e42-1140590e5c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Model instance\n",
    "model = PiNet()\n",
    "\n",
    "# 2. Initialize the parameters of the model\n",
    "key = random.PRNGKey(0)\n",
    "key, init_key = random.split(key)\n",
    "params = model.init(key, jnp.ones([1]))['params'] #change the 3 to match the dimension of input data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f4fd5e-22ed-44d0-ab7b-ccdde9a3edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss_fn(pred, known, params):    \n",
    "    return jnp.sum(jnp.power(known - pred, 2)) #log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb87780a-75d9-41af-aafd-acee33fbc160",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def calculate_loss(params, t, y_known):\n",
    "  \n",
    "    y_pred = model.apply({'params': params}, t)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y_known, params)\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a138fb9-9e76-4f1c-b8fa-4297c353fa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def calculate_value_loss_grad(params, t, y_known):\n",
    "    y_pred = model.apply({'params': params}, t)\n",
    "    loss = loss_fn(y_pred, y_known, params)\n",
    "    \n",
    "    grads = jax.grad(calculate_loss, 0)(params, t, y_known)\n",
    "    \n",
    "    return y_pred, loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68c999-4cde-44e8-8715-d6d14d75db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F. Initial train state including parameters initialization\n",
    "def create_train_state(key, lr=5e-2):\n",
    "    \"\"\"Creates initial `TrainState for our classifier.\n",
    "    \n",
    "    Args:\n",
    "        key: PRNG key to initialize the model parameters\n",
    "        lr: Learning rate for the optimizer\n",
    "    \n",
    "    \"\"\"\n",
    "    # 1. Model instance\n",
    "    model = PiNet()\n",
    "    \n",
    "    # 2. Initialize the parameters of the model\n",
    "    params = model.init(key, jnp.ones([1]))['params'] #change the 3 to match the dimension of input data...  \n",
    "    \n",
    "    # 3. Define the optimizer with the desired learning rate\n",
    "    #constant learning rate:\n",
    "    optimizer = optax.adam(learning_rate=lr) #lr passed in from function\n",
    "    \n",
    "    # 4. Create and return initial state from the above information. The `Module.apply` applies a \n",
    "    # module method to variables and returns output and modified variables.\n",
    "    return model, train_state.TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdde8a0-3fba-49d3-8323-f74deb716e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize model parameters\n",
    "key = random.PRNGKey(0)\n",
    "key, init_key = random.split(key)\n",
    "model, state = create_train_state(init_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3798257-b4df-4bd5-926b-e7d97c35114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit #can't jit the nonlinear solver\n",
    "def train_step_gradient_descent(state, t, y_known):\n",
    "    \"\"\"Defines the single training step.\n",
    "    \"\"\"\n",
    "    \n",
    "    #calculate loss, grad\n",
    "    y_pred, loss, grads = jax.vmap(calculate_value_loss_grad, in_axes=(None, 0,0))(state.params, t, true_y)\n",
    "    \n",
    "    #accumulate loss and grad\n",
    "    loss = jnp.sum(loss, 0)\n",
    "    grads = jtu.tree_map(lambda x: jnp.sum(x, 0), grads)\n",
    "    \n",
    "    #update gradients: \n",
    "    lr = 1e-6\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    \n",
    "    \n",
    "    # 5. Return loss, accuracy and the updated state\n",
    "    return y_pred, loss, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8c4dd6-74ac-4bbd-9599-a366ca6c111f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred, loss, state = train_step_gradient_descent(state, t, true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b8747c-9046-4c2e-bfd7-f6ba9ca5d738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3e4ce2",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae3f22-2f90-4314-8b66-3f955762d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "test_freq = 500\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "key, init_key = random.split(key)\n",
    "model, state = create_train_state(init_key)\n",
    "\n",
    "# Lists to record loss for each epoch\n",
    "training_loss = []\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Training \n",
    "for itr in range(EPOCHS):  \n",
    "    y_pred, loss, state = train_step_gradient_descent(state, t, true_y)\n",
    "    \n",
    "    if loss < 1e-17:\n",
    "        break\n",
    "    \n",
    "    training_loss.append(loss)\n",
    "    \n",
    "    if itr % test_freq == 0 or itr == EPOCHS-1:\n",
    "        print('Iter {:04d} | Total Loss {:e}'.format(itr, training_loss[-1]))\n",
    "        \n",
    "        #loss graph -- don't change \n",
    "        #ax = plt.gca()\n",
    "        ax1.cla()\n",
    "        ax1.semilogy(training_loss)\n",
    "        ax1.set_ylabel('training Loss')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.minorticks_on()\n",
    "\n",
    "        #data and NN prediction\n",
    "        ax2.cla()\n",
    "        ax2.plot(t,y_pred, '-')\n",
    "        ax2.scatter(t,true_y)\n",
    "        ax2.set_xlabel('t')\n",
    "        ax2.set_ylabel('y')\n",
    "        ax2.minorticks_on()\n",
    "        ax2.legend('NN', 'data')\n",
    "\n",
    "        fig.tight_layout()\n",
    "        display(fig)\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "    \n",
    "    \n",
    "    #print(f\"loss: {training_loss[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46117b94-f2b4-4e74-8442-3dce767609f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d926e3da-6c55-40fc-95b6-7dc2a97af0f0",
   "metadata": {},
   "source": [
    "### Calculation with gradient\n",
    "\n",
    "$\\mathcal{I}(\\theta) = \\operatorname{E} \\left[\\left. \\left(\\frac{\\partial}{\\partial\\theta} \\log f(X;\\theta)\\right)^2\\right|\\theta \\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3159173a-7bc3-4611-9e45-d8580c573f87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get the gradients\n",
    "ypred, loss, grads = jax.vmap(calculate_value_loss_grad, in_axes=(None, 0,0))(state.params, t, true_y)\n",
    "#jtu.tree_map(lambda x: x.shape, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e4575-b63c-4a36-b548-d8995747b4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_matrix, unravel_fn  = jax.flatten_util.ravel_pytree(state.params)\n",
    "grads_matrix = jax.vmap(lambda x: jax.flatten_util.ravel_pytree(x)[0], (0))(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d5dfec-2cf5-411d-a492-2fd274b4a317",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_std = 1e30\n",
    "cov = jnp.linalg.pinv(grads_matrix.transpose() @ grads_matrix * 1/(2*jnp.var(true_y - ypred, 0, ddof=1)+1e-30)**2 + jnp.eye(grads_matrix.shape[-1]) * 1/prior_std**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4088fc4-2ecd-41a2-9dfe-aef9b4d41c22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2689f518-ec73-40ba-96be-60ce96275291",
   "metadata": {},
   "source": [
    "### Expanding out the Parameters with Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1675dc43-86e3-4d55-81ce-e7cfa9549942",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "samples = np.random.multivariate_normal(mean_matrix, cov, n_samples)\n",
    "sample_params = jax.vmap(unravel_fn)(samples)\n",
    "\n",
    "expanded = []\n",
    "for it in range(0,n_samples):\n",
    "    if it % 100 == 0:\n",
    "        print(it)\n",
    "    sample_param_i = jtu.tree_map(lambda x: x[it], sample_params)\n",
    "    equation = model.get_equation(sample_param_i, ['x'])\n",
    "    sample_expanded = sympy.Poly(equation[0], sympy.symbols('x')).as_dict(sympy.symbols('x')).values()\n",
    "    sample_expanded = np.array(list(sample_expanded), np.float64)\n",
    "    expanded.append(sample_expanded)\n",
    "expanded = np.array(expanded, np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cc1af0-0506-45f5-ac38-3e696fc94fea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#np.save('Laplace_cubic_expanded.npy', expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c808104-0f2a-47a2-9926-1f7393914d0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.kdeplot(expanded[:,0], label='$1$')\n",
    "#plt.axvline(1, 0)\n",
    "sns.kdeplot(expanded[:,1], label='$x$')\n",
    "#plt.axvline(1, 0)\n",
    "sns.kdeplot(expanded[:,2], label='$2x^2$')\n",
    "#plt.axvline(2, 0)\n",
    "sns.kdeplot(expanded[:,3], label='$4x^3$')\n",
    "#plt.axvline(4, 0)\n",
    "plt.xlabel('Parameter Value')\n",
    "plt.legend()\n",
    "plt.minorticks_on()\n",
    "plt.ylabel('Kernel Density Estimate')\n",
    "\n",
    "#plt.savefig('CubicRegression_kde.svg')\n",
    "#plt.savefig('CubicRegression_kde.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92fab20-a9bf-4591-b522-e7f4c360713d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "dim = 1\n",
    "\n",
    "def function2(param_flat, x):\n",
    "    return jnp.dot(jnp.column_stack((jnp.ones_like(t), t, t**2, t**3)), param_flat)\n",
    "\n",
    "params_samples = expanded\n",
    "ys = jax.vmap(function2, (0,None))(params_samples, t)[:,:,None]\n",
    "\n",
    "ys_mean = np.mean(ys, 0)\n",
    "ys_stdev  = np.std(ys, 0)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.scatter(t, true_y, label='Noise Corrupted Training Data')\n",
    "plt.plot(t, true_fun(t).squeeze() , color='g', label=\"True Model\")\n",
    "plt.plot(t, ys_mean , color='r', label=\"Mean Model\")\n",
    "for i in range(0, dim):\n",
    "    plt.fill_between(t.squeeze(), ys_mean[:,i]-2.0*ys_stdev[:,i], ys_mean[:,i]+2.0*ys_stdev[:,i], alpha=0.3,color='royalblue',label='95% confidence interval')\n",
    "    plt.fill_between(t.squeeze(), ys_mean[:,i]+2.0*ys_stdev[:,i], ys_mean[:,i]+3.0*ys_stdev[:,i], alpha=0.3,color='aqua',label='99.7% confidence interval')\n",
    "    plt.fill_between(t.squeeze(), ys_mean[:,i]-2.0*ys_stdev[:,i], ys_mean[:,i]-3.0*ys_stdev[:,i], alpha=0.3,color='aqua')\n",
    "    \n",
    "plt.legend(fontsize=12)\n",
    "plt.xlabel('x')\n",
    "plt.minorticks_on()\n",
    "plt.ylabel('y')\n",
    "#plt.savefig('CubicRegression_Laplace_Uncertainty_Figure.svg')\n",
    "#plt.savefig('CubicRegression_Laplace_Uncertainty_Figure.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541a2e34-9af7-4e0b-83ba-b614ba79226e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db15d993-82d5-4816-b1de-757050113eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
