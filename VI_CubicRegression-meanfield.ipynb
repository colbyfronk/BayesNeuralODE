{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a0c03e4-686e-4e1f-8f8d-1eee7977a409",
   "metadata": {},
   "source": [
    "### Bayesian Cubic Regression with Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b93ec61-1387-486f-a038-6ab8d34aeb8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.integrate import solve_ivp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from jax import make_jaxpr\n",
    "from jax.config import config\n",
    "from jax import value_and_grad\n",
    "from jax import grad, vmap, pmap, jit\n",
    "import jax.tree_util as jtu\n",
    "\n",
    "import optax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import flax\n",
    "\n",
    "from typing import Any, Callable, Sequence, Optional\n",
    "import sympy\n",
    "\n",
    "import distrax\n",
    "\n",
    "from sympy import Matrix\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import functools\n",
    "\n",
    "from NN_arch import PiNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410259fa-cf07-419d-a9b7-d289a9ed5598",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3695b4-acad-450d-a392-87567b2c20e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\n",
    "warnings.filterwarnings(\"ignore\", \"use_inf_as_na\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2250d7-dc48-4741-b2e1-e9dfa73191d0",
   "metadata": {},
   "source": [
    "### Cubic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9b8e75-000c-4da8-a221-a16e739bf698",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define True Model Function and Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a2d28-94ae-4809-bf6e-29e1f4758567",
   "metadata": {},
   "source": [
    "$ y = 1 + t + 2t^2 + 4t^3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9325e3f-f29c-425b-8327-6785e30672a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ndata = 200 #number of known data points\n",
    "\n",
    "t0 = -1.25\n",
    "t1 = 1.25\n",
    "t = jnp.linspace(t0, t1, ndata)\n",
    "\n",
    "def true_fun(t):\n",
    "    return jnp.array([1 + t + 2*t**2 + 4*t**3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0984a9-64b7-47b0-8e38-a2df8b06f80f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stdev = 3\n",
    "\n",
    "seed = 989\n",
    "np.random.seed(seed)\n",
    "\n",
    "true_y = true_fun(t).squeeze() \n",
    "true_y = true_y + np.random.normal(scale=stdev, size=true_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee769b-aa36-4fcb-b0e7-deaac897fb83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the dataset\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(t, true_fun(t).squeeze() , color='r', label=\"True values\")\n",
    "plt.scatter(t, true_y, label=\"Noise corrupted values\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Labels\")\n",
    "plt.title(\"Real function along with noisy targets\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8d3ec4-4b9c-47e4-9ece-094964ac1ae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = t[:,None]\n",
    "true_y = true_y[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a69dd59-eb16-4a1d-868a-9e596a079899",
   "metadata": {},
   "source": [
    "### Pre-train Neural ODE for guess values of VI initial parameters.  \n",
    "\n",
    "This speeds up the method significantly and improves the accuracy, but is not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814ff8e5-f40a-454c-bcf8-e2ba90dadc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Model instance\n",
    "model = PiNet()\n",
    "\n",
    "# 2. Initialize the parameters of the model\n",
    "key = random.PRNGKey(0)\n",
    "key, init_key = random.split(key)\n",
    "params = model.init(key, jnp.ones([1]))['params'] #change the 3 to match the dimension of input data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10011198-8ae1-4f00-b85b-1864336643bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss_fn(pred, known, params):    \n",
    "    return jnp.sum(jnp.power(known - pred, 2)) #log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd62e75f-13ef-49b3-afe2-e30dde530bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def calculate_loss(params, t, y_known):\n",
    "  \n",
    "    y_pred = model.apply({'params': params}, t)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y_known, params)\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bbcfe0-9a01-4fa4-a09f-24b7ea95f6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def calculate_value_loss_grad(params, t, y_known):\n",
    "    y_pred = model.apply({'params': params}, t)\n",
    "    loss = loss_fn(y_pred, y_known, params)\n",
    "    \n",
    "    grads = jax.grad(calculate_loss, 0)(params, t, y_known)\n",
    "    \n",
    "    return y_pred, loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cdf2fc-a76f-4bcd-8914-5b05ec8c0558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F. Initial train state including parameters initialization\n",
    "def create_train_state(key, lr=5e-2):\n",
    "    \"\"\"Creates initial `TrainState for our classifier.\n",
    "    \n",
    "    Args:\n",
    "        key: PRNG key to initialize the model parameters\n",
    "        lr: Learning rate for the optimizer\n",
    "    \n",
    "    \"\"\"\n",
    "    # 1. Model instance\n",
    "    model = PiNet()\n",
    "    \n",
    "    # 2. Initialize the parameters of the model\n",
    "    params = model.init(key, jnp.ones([1]))['params'] #change the 3 to match the dimension of input data...  \n",
    "    \n",
    "    # 3. Define the optimizer with the desired learning rate\n",
    "    #constant learning rate:\n",
    "    optimizer = optax.adam(learning_rate=lr) #lr passed in from function\n",
    "    \n",
    "    # 4. Create and return initial state from the above information. The `Module.apply` applies a \n",
    "    # module method to variables and returns output and modified variables.\n",
    "    return model, train_state.TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ccf1db-c6ff-4e43-be95-0bfa752d8c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize model parameters\n",
    "key = random.PRNGKey(0)\n",
    "key, init_key = random.split(key)\n",
    "model, state = create_train_state(init_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6c1f98-9008-4199-9c59-f31f7ca6985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit #can't jit the nonlinear solver\n",
    "def train_step_gradient_descent(state, t, y_known):\n",
    "    \"\"\"Defines the single training step.\n",
    "    \"\"\"\n",
    "    \n",
    "    #calculate loss, grad\n",
    "    y_pred, loss, grads = jax.vmap(calculate_value_loss_grad, in_axes=(None, 0,0))(state.params, t, true_y)\n",
    "    \n",
    "    #accumulate loss and grad\n",
    "    loss = jnp.sum(loss, 0)\n",
    "    grads = jtu.tree_map(lambda x: jnp.sum(x, 0), grads)\n",
    "    \n",
    "    #update gradients: \n",
    "    lr = 1e-6\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    \n",
    "    \n",
    "    # 5. Return loss, accuracy and the updated state\n",
    "    return y_pred, loss, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede5b92-bbe9-4e3e-a929-4fb647fd78cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "test_freq = 500\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "key, init_key = random.split(key)\n",
    "model, state = create_train_state(init_key)\n",
    "\n",
    "# Lists to record loss for each epoch\n",
    "training_loss = []\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Training \n",
    "for itr in range(EPOCHS):  \n",
    "    y_pred, loss, state = train_step_gradient_descent(state, t, true_y)\n",
    "    \n",
    "    if loss < 1e-17:\n",
    "        break\n",
    "    \n",
    "    training_loss.append(loss)\n",
    "    \n",
    "    if itr % test_freq == 0 or itr == EPOCHS-1:\n",
    "        print('Iter {:04d} | Total Loss {:e}'.format(itr, training_loss[-1]))\n",
    "        \n",
    "        #loss graph -- don't change \n",
    "        #ax = plt.gca()\n",
    "        ax1.cla()\n",
    "        ax1.semilogy(training_loss)\n",
    "        ax1.set_ylabel('training Loss')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.minorticks_on()\n",
    "\n",
    "        #data and NN prediction\n",
    "        ax2.cla()\n",
    "        ax2.plot(t,y_pred, '-')\n",
    "        ax2.scatter(t,true_y)\n",
    "        ax2.set_xlabel('t')\n",
    "        ax2.set_ylabel('y')\n",
    "        ax2.minorticks_on()\n",
    "        ax2.legend('NN', 'data')\n",
    "\n",
    "        fig.tight_layout()\n",
    "        display(fig)\n",
    "        \n",
    "        print(model.get_equation(state.params, ['x']))\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "    \n",
    "    \n",
    "    #print(f\"loss: {training_loss[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06195f30-29c1-450e-9625-908f08b9e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_pretrained = state.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21510c2f-5990-4f6d-809d-d33bb27a4d31",
   "metadata": {},
   "source": [
    "### Setup BNN functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116c97b2-8082-4b09-ba05-177f9f1c606f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Model instance\n",
    "model = PiNet()\n",
    "\n",
    "# 2. Initialize the parameters of the model\n",
    "key = jax.random.PRNGKey(314)\n",
    "key, init_key = jax.random.split(key)\n",
    "\n",
    "#params_mu = model.init(key, jnp.ones([1]))['params'] #change the 3 to match the dimension of input data...\n",
    "params_mu = params_pretrained\n",
    "params_stdev = jtu.tree_map(lambda x: jnp.ones_like(x) * 5, params_mu)\n",
    "params = flax.core.frozen_dict.freeze({'mu': params_mu, 'stdev': params_stdev})\n",
    "\n",
    "prior_mu = jtu.tree_map(lambda x: jnp.zeros_like(x), params_mu)\n",
    "prior_stdev = jtu.tree_map(lambda x: jnp.ones_like(x) * 100000, params_mu)\n",
    "prior_params = flax.core.frozen_dict.freeze({'mu': prior_mu, 'stdev': prior_stdev})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f20d1a-93d4-4adb-805f-d9aa17e8a51a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@jax.jit\n",
    "def sample_params(params, key, Nsamples=100):\n",
    "    \"\"\"\n",
    "    return a matrix of size (D, K) containing K samples from\n",
    "     our variational distribution q\n",
    "    \"\"\"\n",
    "    eps = jtu.tree_map(lambda x: jax.random.normal(key, shape=((Nsamples,) + x.shape)), params['mu'])\n",
    "    #print(jtu.tree_map(lambda x: x.shape, eps))\n",
    "    #print(jtu.tree_map(lambda x: x.shape, params['mu']))\n",
    "    #print(jtu.tree_map(lambda x: x.shape, params['stdev']))\n",
    "    \n",
    "    #w = jtu.tree_map(lambda x, y, z: x + jnp.abs(y) * z, params['mu'], params['stdev'], eps)\n",
    "    w = jtu.tree_map(lambda x, y, z: x + jnp.abs(y) * z, params['mu'], params['stdev'], eps)\n",
    "    #print(jtu.tree_map(lambda x: x.shape, w))\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d2aba2-82f9-4a21-b6fb-f8db48fd63df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jtu.tree_map(lambda x: x.shape, sample_params(prior_params, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f950b78e-6167-4bdd-b724-5a414c1d3a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def KLD_cost(q_params, p_params):\n",
    "    q_mu = q_params['mu']\n",
    "    q_stdev = q_params['stdev']\n",
    "    p_mu = p_params['mu']\n",
    "    p_stdev = p_params['stdev']\n",
    "    \n",
    "    #p_logdet\n",
    "    leaves, _ = jtu.tree_flatten(jtu.tree_map(lambda x: jnp.log(x**2), p_stdev))\n",
    "    p_logdet = jnp.clip(jnp.sum(jnp.array([jnp.sum(leaf) for leaf in leaves])), a_min=-700)\n",
    "    #print(p_logdet)\n",
    "    \n",
    "    #q_logdet\n",
    "    leaves, _ = jtu.tree_flatten(jtu.tree_map(lambda x: jnp.log(x**2), q_stdev))\n",
    "    q_logdet = jnp.clip(jnp.sum(jnp.array([jnp.sum(leaf) for leaf in leaves])), a_min=-700)\n",
    "    #print(q_logdet)\n",
    "    \n",
    "    logdet_ratio = p_logdet - q_logdet\n",
    "    #print(logdet_ratio)\n",
    "    \n",
    "    #k = number of parameters:\n",
    "    leaves, _ = jtu.tree_flatten(jtu.tree_map(lambda x: x.size, q_mu))\n",
    "    k = sum(leaves)\n",
    "    #print(k)\n",
    "    \n",
    "    #dmu_sigma_inv_dmu\n",
    "    dmu = jtu.tree_map(lambda x,y: x - y, q_mu, p_mu)\n",
    "    dmu_sigma_inv_dmu = jtu.tree_map(lambda x, y: x**2 / y**2, dmu, p_stdev)\n",
    "    #print(dmu_sigma_inv_dmu)\n",
    "    leaves, _ = jtu.tree_flatten(dmu_sigma_inv_dmu)\n",
    "    dmu_sigma_inv_dmu = sum([jnp.sum(leaf) for leaf in leaves])\n",
    "    #print(dmu_sigma_inv_dmu)\n",
    "    \n",
    "    #trace term\n",
    "    trace_term = jtu.tree_map(lambda x,y: (x**2)/(y**2), q_stdev, p_stdev)\n",
    "    #print(trace_term)\n",
    "    leaves, _ = jtu.tree_flatten(trace_term)\n",
    "    trace_term = sum([jnp.sum(leaf) for leaf in leaves])\n",
    "    #print(trace_term)\n",
    "    \n",
    "    KLD = 0.5 * (logdet_ratio - k + dmu_sigma_inv_dmu + trace_term)\n",
    "    \n",
    "    return KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f171b115-5105-462e-a8d2-ead21b81119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "KLD_cost(params, prior_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d754e744-bc43-44de-ad7e-771592370620",
   "metadata": {},
   "outputs": [],
   "source": [
    "KLD_cost(prior_params, prior_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f019102-73b1-42aa-b117-e9093be626b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss_fn(pred, known, params):    \n",
    "    unregularized_loss = jnp.power(pred - known, 2)\n",
    "    return unregularized_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ada8823-29a8-46a9-9b4d-c2628e5f2c39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def ELBO(params, X, known, key, Nsamples=1000):\n",
    "    sampled_params = sample_params(params, key, Nsamples=Nsamples)\n",
    "    #print(jtu.tree_map(lambda x: x.shape, sampled_params))\n",
    "\n",
    "    #calculate log likelihood:\n",
    "    ypred = jax.vmap(jax.vmap(model.apply, (None, 0)), (0,None))({'params': sampled_params}, X)\n",
    "    print(ypred.shape)\n",
    "    ll_term = jnp.mean(loss_fn(ypred, known, sampled_params),0).sum()\n",
    "    print(ll_term)\n",
    "\n",
    "    KL_term = KLD_cost(params, prior_params)\n",
    "    #print(KL_term)\n",
    "    \n",
    "    return -1*(-ll_term.mean() - KL_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eab150-129a-405e-8f79-91dcbd2bc1f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ELBO(params, t, true_y, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f8b720-9be0-4f97-bad7-70e8c28fe960",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# F. Initial train state including parameters initialization\n",
    "def create_train_state(key, init_params, lr=1e-3):\n",
    "    \"\"\"Creates initial `TrainState for our classifier.\n",
    "    \n",
    "    Args:\n",
    "        key: PRNG key to initialize the model parameters\n",
    "        lr: Learning rate for the optimizer\n",
    "    \n",
    "    \"\"\"\n",
    "    # 1. Model instance\n",
    "    model = PiNet()\n",
    "    \n",
    "    # 2. Initialize the parameters of the model    \n",
    "    #params_mu = model.init(key, jnp.ones([1]))['params'] #change the 3 to match the dimension of input data...\n",
    "    params_mu = init_params\n",
    "    params_stdev = jtu.tree_map(lambda x: jnp.ones_like(x) * 0.5, params_mu)\n",
    "    params = flax.core.frozen_dict.freeze({'mu': params_mu, 'stdev': params_stdev})\n",
    "\n",
    "    prior_mu = jtu.tree_map(lambda x: jnp.zeros_like(x), params_mu)\n",
    "    prior_stdev = jtu.tree_map(lambda x: jnp.ones_like(x) * 100000, params_mu)\n",
    "    prior_params = flax.core.frozen_dict.freeze({'mu': prior_mu, 'stdev': prior_stdev})\n",
    "    \n",
    "    # 3. Define the optimizer with the desired learning rate\n",
    "    #constant learning rate:\n",
    "    optimizer = optax.adam(learning_rate=lr) #lr passed in from function\n",
    "    \n",
    "    # 4. Create and return initial state from the above information. The `Module.apply` applies a \n",
    "    # module method to variables and returns output and modified variables.\n",
    "    return model, train_state.TrainState.create(apply_fn=model.apply, params=params, tx=optimizer), prior_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b00d65f-7a3c-4973-b4ce-295d627fab25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#initialize model parameters\n",
    "key = jax.random.PRNGKey(0)\n",
    "key, init_key = jax.random.split(key)\n",
    "#init_params = model.init(key, jnp.ones([1]))['params']\n",
    "init_params = params_pretrained\n",
    "model, state, prior_params = create_train_state(init_key, init_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c812bb82-688c-43dc-91ef-ceff090e411d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jax.jit \n",
    "def train_step_gradient_descent(state, t, y_known, key):\n",
    "    \"\"\"Defines the single training step.\n",
    "    \"\"\"\n",
    "    sampled_params = sample_params(state.params, key, Nsamples=1000)\n",
    "    ypred = jax.vmap(jax.vmap(model.apply, (None, 0)), (0,None))({'params': sampled_params}, t)\n",
    "    \n",
    "    loss, grads = jax.value_and_grad(ELBO)(state.params, t, y_known, key)\n",
    "\n",
    "    #mu_zeros = jtu.tree_map(lambda x: jnp.zeros_like(x), grads['mu'])\n",
    "    #grads = flax.core.frozen_dict.unfreeze(grads)\n",
    "    #grads['mu'] = mu_zeros\n",
    "    #grads = flax.core.frozen_dict.freeze(grads)\n",
    "    #print(grads)\n",
    "    \n",
    "    #update gradients: \n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    \n",
    "    # 5. Return loss, accuracy and the updated state\n",
    "    return ypred, loss, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd7edfc-64e5-4ada-ab45-ca6f5647d937",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ypred, loss, state = train_step_gradient_descent(state, t, true_y, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48646f34-2980-45b8-ba29-1659b875f67e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ypred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed31cbaa-e661-4c52-86a0-360ba94bb19f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ba81e-1642-4dc0-a79e-5a9e4a207b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177d6d05-aea1-4087-b482-4d591eb553f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a41a96c-61ec-452a-8d93-01f8b591d38d",
   "metadata": {},
   "source": [
    "### Train BNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf57197-a81a-4c73-a9af-fffa26aac293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 2000\n",
    "test_freq = 500\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "key, init_key = random.split(key)\n",
    "model, state, prior_params = create_train_state(init_key, init_params)\n",
    "\n",
    "# Lists to record loss for each epoch\n",
    "training_loss = []\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Training \n",
    "for itr in range(EPOCHS):  \n",
    "    key, init_key = jax.random.split(key)\n",
    "    ypred, loss, state = train_step_gradient_descent(state, t, true_y, key)\n",
    "    \n",
    "    #if loss < 1e-17:\n",
    "    #    break\n",
    "    \n",
    "    training_loss.append(loss)\n",
    "    \n",
    "    if itr % test_freq == 0 or itr == EPOCHS-1:\n",
    "        print('Iter {:04d} | Total Loss {:e}'.format(itr, training_loss[-1]))\n",
    "        \n",
    "        #loss graph -- don't change \n",
    "        #ax = plt.gca()\n",
    "        ax1.cla()\n",
    "        ax1.semilogy(training_loss)\n",
    "        ax1.set_ylabel('training Loss')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.minorticks_on()\n",
    "\n",
    "        #data and NN prediction\n",
    "        ax2.cla()\n",
    "        ax2.plot(t,jnp.mean(ypred,0), '-')\n",
    "        ax2.scatter(t,true_y)\n",
    "        ax2.fill_between(t.squeeze(), jnp.mean(ypred,0).squeeze()-3.0*jnp.var(ypred, 0).squeeze()**0.5, jnp.mean(ypred,0).squeeze()+3.0*jnp.var(ypred, 0).squeeze()**0.5, alpha=0.3,color='aqua')\n",
    "        ax2.set_xlabel('t')\n",
    "        ax2.set_ylabel('y')\n",
    "        ax2.minorticks_on()\n",
    "        ax2.legend('NN', 'data')\n",
    "\n",
    "        fig.tight_layout()\n",
    "        display(fig)\n",
    "        \n",
    "        #print(model.get_equation(state.params, ['x']))\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "    \n",
    "    \n",
    "    #print(f\"loss: {training_loss[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3e00c2-6f09-4dc3-a141-785093db8359",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08ce64a-a454-457c-b923-cb540d9367d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "sampled_params = sample_params(state.params, key, Nsamples=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f37d9c-32c8-4a94-972b-ed811cd789a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dim = 1\n",
    "ys = np.zeros((n_samples, ndata, dim))\n",
    "for i in range(0, n_samples):\n",
    "    params_i = jtu.tree_map(lambda x: x[i], sampled_params)\n",
    "    y_i = jax.vmap(functools.partial(model.apply, {'params': params_i}), (0))(t)\n",
    "    ys[i] = np.array(y_i)\n",
    "\n",
    "ys_mean = np.mean(ys, 0)\n",
    "ys_stdev  = np.std(ys, 0)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.scatter(t, true_y, label='Noise Corrupted Training Data')\n",
    "plt.plot(t, true_fun(t).squeeze() , color='g', label=\"True Model\")\n",
    "plt.plot(t, ys_mean , color='r', label=\"Mean Model\")\n",
    "for i in range(0, dim):\n",
    "    plt.fill_between(t.squeeze(), ys_mean[:,i]-3.0*ys_stdev[:,i], ys_mean[:,i]+3.0*ys_stdev[:,i], alpha=0.3,color='royalblue',label='99.7% CI')\n",
    "    plt.fill_between(t.squeeze(), ys_mean[:,i]-2.0*ys_stdev[:,i], ys_mean[:,i]-3.0*ys_stdev[:,i], alpha=0.3,color='aqua',label='99.7% CI')\n",
    "    plt.fill_between(t.squeeze(), ys_mean[:,i]+2.0*ys_stdev[:,i], ys_mean[:,i]+3.0*ys_stdev[:,i], alpha=0.3,color='aqua') #,label='99.7% confidence interval')\n",
    "    \n",
    "plt.legend(fontsize=12)\n",
    "plt.xlabel('x')\n",
    "plt.minorticks_on()\n",
    "plt.ylabel('y')\n",
    "#plt.savefig('Uncertainty_Figure.svg')\n",
    "#plt.savefig('Uncertainty_Figure.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeae09b-032c-42d1-bbaf-d898b8a75050",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "fig, axs = plt.subplots(3, 3, figsize=(12, 12), sharey=False)\n",
    "\n",
    "keys = list(sampled_params.keys())\n",
    "\n",
    "sns.kdeplot(ax = axs[0,0], data=sampled_params[keys[0]], label=None, legend=False)\n",
    "axs[0,0].set_title(keys[0])\n",
    "axs[0,0].set_ylabel('Kernel Density Estimate')\n",
    "\n",
    "sns.kdeplot(ax = axs[0,1], data=sampled_params[keys[1]].squeeze(), label=None, legend=False)\n",
    "axs[0,1].set_title(keys[1])\n",
    "axs[0,1].set_ylabel(None)\n",
    "\n",
    "sns.kdeplot(ax = axs[0,2], data=sampled_params[keys[2]].squeeze(), label=None, legend=False)\n",
    "axs[0,2].set_title(keys[2])\n",
    "axs[0,2].set_ylabel(None)\n",
    "\n",
    "sns.kdeplot(ax = axs[1,0], data=sampled_params[keys[3]].squeeze(), label=None, legend=False)\n",
    "axs[1,0].set_title(keys[3])\n",
    "axs[1,0].set_ylabel('Kernel Density Estimate')\n",
    "\n",
    "sns.kdeplot(ax = axs[1,1], data=sampled_params[keys[4]].squeeze(), label=None, legend=False)\n",
    "axs[1,1].set_title(keys[4])\n",
    "axs[1,1].set_ylabel(None)\n",
    "\n",
    "sns.kdeplot(ax = axs[1,2], data=sampled_params[keys[5]][:,0].squeeze(), label=None, legend=False)\n",
    "axs[1,2].set_title(keys[5])\n",
    "axs[1,2].set_ylabel(None)\n",
    "\n",
    "sns.kdeplot(ax = axs[2,0], data=sampled_params[keys[6]].squeeze(), label=None, legend=False)\n",
    "axs[2,0].set_title(keys[6])\n",
    "axs[2,0].set_ylabel('Kernel Density Estimate')\n",
    "axs[2,0].set_xlabel('Parameter Value')\n",
    "\n",
    "sns.kdeplot(ax = axs[2,1], data=sampled_params[keys[7]].squeeze(), label=None, legend=False)\n",
    "axs[2,1].set_title(keys[7])\n",
    "axs[2,1].set_ylabel(None)\n",
    "axs[2,1].set_xlabel('Parameter Value')\n",
    "\n",
    "sns.kdeplot(ax = axs[2,2], data=sampled_params[keys[8]].squeeze(), label=None, legend=False)\n",
    "axs[2,2].set_title(keys[8])\n",
    "axs[2,2].set_ylabel(None)\n",
    "axs[2,2].set_xlabel('Parameter Value')\n",
    "\n",
    "for ax in axs.flatten():\n",
    "    ax.minorticks_on()\n",
    "    ax.set_ylim([0,1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Cubic_Posteriors_VI_w&b.svg')\n",
    "plt.savefig('Cubic_Posteriors_VI_w&b.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b59fe-3d3b-455c-bc0e-2bd0c49852c5",
   "metadata": {},
   "source": [
    "### Expanding out the Parameters with Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139b227b-0ab6-4916-b081-1fff0072a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 500\n",
    "sampled_params = sample_params(state.params, key, Nsamples=n_samples)\n",
    "expanded = []\n",
    "it = 0\n",
    "for i in range(0, n_samples):\n",
    "    it = it + 1\n",
    "    if it % 100 == 0:\n",
    "        print(it)\n",
    "    sample_params = jtu.tree_map(lambda x: x[i], sampled_params)\n",
    "    equation = model.get_equation(sample_params, ['x'])\n",
    "    sample_expanded = sympy.Poly(equation[0], sympy.symbols('x')).as_dict(sympy.symbols('x')).values()\n",
    "    sample_expanded = np.array(list(sample_expanded), np.float64)\n",
    "    expanded.append(sample_expanded)\n",
    "expanded = np.array(expanded, np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97042d44-a2da-4eda-90d8-a519433d6b93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#np.save('VI_cubic_expanded.npy', expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22d9e8a-a07a-497a-bfb2-07537f11a298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.kdeplot(expanded[:,0], label='$1$')\n",
    "#plt.axvline(1, 0)\n",
    "sns.kdeplot(expanded[:,1], label='$x$')\n",
    "#plt.axvline(1, 0)\n",
    "sns.kdeplot(expanded[:,2], label='$2x^2$')\n",
    "#plt.axvline(2, 0)\n",
    "sns.kdeplot(expanded[:,3], label='$4x^3$')\n",
    "#plt.axvline(4, 0)\n",
    "plt.xlabel('Parameter Value')\n",
    "plt.legend()\n",
    "plt.minorticks_on()\n",
    "plt.ylabel('Kernel Density Estimate')\n",
    "\n",
    "plt.savefig('CubicRegression_VI_kde.svg')\n",
    "plt.savefig('CubicRegression_VI_kde.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e66999-afd2-431c-9276-07c16fa40723",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 1\n",
    "\n",
    "def function2(param_flat, x):\n",
    "    return jnp.dot(jnp.column_stack((jnp.ones_like(t), t, t**2, t**3)), param_flat)\n",
    "\n",
    "params_samples = expanded\n",
    "ys = jax.vmap(function2, (0,None))(params_samples, t)[:,:,None]\n",
    "\n",
    "ys_mean = np.mean(ys, 0)\n",
    "ys_stdev  = np.std(ys, 0)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.scatter(t, true_y, label='Noise Corrupted Training Data')\n",
    "plt.plot(t, true_fun(t).squeeze() , color='g', label=\"True Model\")\n",
    "plt.plot(t, ys_mean , color='r', label=\"Mean Model\")\n",
    "for i in range(0, dim):\n",
    "    plt.fill_between(t.squeeze(), ys_mean[:,i]-2.0*ys_stdev[:,i], ys_mean[:,i]+2.0*ys_stdev[:,i], alpha=0.3,color='royalblue',label='95% confidence interval')\n",
    "    plt.fill_between(t.squeeze(), ys_mean[:,i]+2.0*ys_stdev[:,i], ys_mean[:,i]+3.0*ys_stdev[:,i], alpha=0.3,color='aqua',label='99.7% confidence interval')\n",
    "    plt.fill_between(t.squeeze(), ys_mean[:,i]-2.0*ys_stdev[:,i], ys_mean[:,i]-3.0*ys_stdev[:,i], alpha=0.3,color='aqua')\n",
    "    \n",
    "plt.legend(fontsize=12)\n",
    "plt.xlabel('x')\n",
    "plt.minorticks_on()\n",
    "plt.ylabel('y')\n",
    "plt.savefig('CubicRegression_VI_Uncertainty_Figure.svg')\n",
    "plt.savefig('CubicRegression_VI_Uncertainty_Figure.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b51d2-babc-439c-9998-84b2986c6976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688f3bbe-18ec-41a1-b7e5-736aac31e76c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
